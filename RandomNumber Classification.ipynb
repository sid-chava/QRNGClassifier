{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "wgVYr5-TXm5t"
      },
      "outputs": [],
      "source": [
        "# Importing the required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from math import log2, sqrt\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.fft import fft, ifft\n",
        "from scipy.special import erfc\n",
        "from pydotplus import graph_from_dot_data\n",
        "import itertools\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MxSPTrNXsp6",
        "outputId": "0e8f768e-776e-480f-f290-0725176cabe7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       label  spectral_test  shannon_entropy  frequency_test  runs_test  \\\n",
            "0          1     -21.771553         1.935451        0.423711   0.120217   \n",
            "1          1     -22.230385         1.963615        0.841481   0.027240   \n",
            "2          1     -22.230385         1.939471        0.109599   0.498506   \n",
            "3          1     -22.230385         1.872164        0.071861   0.620874   \n",
            "4          1     -22.230385         1.976281        0.230139   0.725698   \n",
            "...      ...            ...              ...             ...        ...   \n",
            "13995      4     -22.230385         1.942653        0.689157   0.556584   \n",
            "13996      4     -22.230385         1.919479        0.689157   0.987149   \n",
            "13997      4     -22.230385         1.862236        0.317311   0.011137   \n",
            "13998      4     -22.230385         1.856367        0.841481   0.548989   \n",
            "13999      4     -22.230385         1.717977        0.071861   0.051254   \n",
            "\n",
            "       autocorrelation  min_entropy    0    1    2  ...   90   91   92   93  \\\n",
            "0                 0.33  -325.550555  0.0  1.0  0.0  ...  1.0  1.0  1.0  1.0   \n",
            "1                 0.31  -325.686601  0.0  1.0  1.0  ...  0.0  1.0  1.0  0.0   \n",
            "2                 0.32  -329.021571  1.0  1.0  1.0  ...  0.0  1.0  1.0  0.0   \n",
            "3                 0.36  -329.009696  1.0  1.0  0.0  ...  1.0  1.0  0.0  1.0   \n",
            "4                 0.18  -295.653057  0.0  0.0  0.0  ...  0.0  0.0  1.0  1.0   \n",
            "...                ...          ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
            "13995             0.24  -329.022753  1.0  1.0  1.0  ...  0.0  0.0  1.0  1.0   \n",
            "13996             0.23  -325.564736  0.0  1.0  0.0  ...  0.0  1.0  0.0  0.0   \n",
            "13997             0.36  -329.008531  1.0  1.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
            "13998             0.25  -329.009840  1.0  1.0  0.0  ...  1.0  1.0  0.0  0.0   \n",
            "13999             0.21  -322.378897  0.0  0.0  1.0  ...  0.0  0.0  0.0  0.0   \n",
            "\n",
            "        94   95   96   97   98   99  \n",
            "0      1.0  1.0  0.0  0.0  1.0  0.0  \n",
            "1      0.0  0.0  1.0  1.0  0.0  1.0  \n",
            "2      0.0  0.0  0.0  0.0  1.0  1.0  \n",
            "3      1.0  1.0  1.0  1.0  0.0  1.0  \n",
            "4      1.0  1.0  0.0  0.0  1.0  1.0  \n",
            "...    ...  ...  ...  ...  ...  ...  \n",
            "13995  0.0  0.0  1.0  1.0  0.0  0.0  \n",
            "13996  0.0  0.0  1.0  1.0  0.0  0.0  \n",
            "13997  1.0  1.0  1.0  0.0  0.0  0.0  \n",
            "13998  1.0  1.0  0.0  1.0  0.0  0.0  \n",
            "13999  1.0  1.0  0.0  1.0  1.0  1.0  \n",
            "\n",
            "[14000 rows x 107 columns]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import log2, sqrt\n",
        "from scipy.fft import fft\n",
        "from scipy.special import erfc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "# Concatenate data\n",
        "def concatenateData(df, num_concats):\n",
        "    new_df = pd.DataFrame({\n",
        "        'Concatenated_Data': [''] * (len(df) // num_concats), \n",
        "        'label': [0] * (len(df) // num_concats)\n",
        "    })\n",
        "\n",
        "    # Loop through each group of num_concats rows and concatenate their 'binary_number' strings\n",
        "    for i in range(0, len(df), num_concats):\n",
        "        new_df.iloc[i // num_concats, 0] = ''.join(df['binary_number'][i:i + num_concats])\n",
        "        new_df.iloc[i // num_concats, 1] = df['label'][i]\n",
        "\n",
        "    return new_df\n",
        "\n",
        "# Calculate Shannon entropy for each concatenated binary sequence\n",
        "def shannon_entropy(binary_string):\n",
        "    if len(binary_string) % 2 != 0:\n",
        "        raise ValueError(\"Binary string length must be a multiple of 2.\")\n",
        "    \n",
        "    patterns = ['00', '10', '11', '01']\n",
        "    frequency = {pattern: 0 for pattern in patterns}\n",
        "    \n",
        "    for i in range(0, len(binary_string), 2):\n",
        "        segment = binary_string[i:i+2]\n",
        "        if segment in patterns:\n",
        "            frequency[segment] += 1\n",
        "    \n",
        "    total_segments = sum(frequency.values())\n",
        "    \n",
        "    entropy = 0\n",
        "    for count in frequency.values():\n",
        "        if count > 0:\n",
        "            probability = count / total_segments\n",
        "            entropy -= probability * log2(probability)\n",
        "    \n",
        "    return entropy\n",
        "\n",
        "\n",
        "def classic_spectral_test(bit_string):\n",
        "    bit_array = 2 * np.array([int(bit) for bit in bit_string]) - 1\n",
        "    dft = fft(bit_array)\n",
        "    n_half = len(bit_string) // 2 + 1\n",
        "    mod_dft = np.abs(dft[:n_half])\n",
        "    threshold = np.sqrt(np.log(1 / 0.05) / len(bit_string))\n",
        "    peaks_below_threshold = np.sum(mod_dft < threshold)\n",
        "    expected_peaks = 0.95 * n_half\n",
        "    d = (peaks_below_threshold - expected_peaks) / np.sqrt(len(bit_string) * 0.95 * 0.05)\n",
        "    p_value = erfc(np.abs(d) / np.sqrt(2)) / 2\n",
        "    return d\n",
        "\n",
        "def frequency_test(bit_string):\n",
        "    n = len(bit_string)\n",
        "    count_ones = bit_string.count('1')\n",
        "    count_zeros = bit_string.count('0')\n",
        "    \n",
        "    # The test statistic\n",
        "    s = (count_ones - count_zeros) / sqrt(n)\n",
        "    \n",
        "    # The p-value\n",
        "    p_value = erfc(abs(s) / sqrt(2))\n",
        "    \n",
        "    return p_value\n",
        "\n",
        "def runs_test(bit_string):\n",
        "    n = len(bit_string)\n",
        "    runs = 1  # Start with the first run\n",
        "    for i in range(1, n):\n",
        "        if bit_string[i] != bit_string[i - 1]:\n",
        "            runs += 1\n",
        "    \n",
        "    n0 = bit_string.count('0')\n",
        "    n1 = bit_string.count('1')\n",
        "    \n",
        "    # Expected number of runs\n",
        "    expected_runs = (2 * n0 * n1 / n) + 1\n",
        "    variance_runs = (2 * n0 * n1 * (2 * n0 * n1 - n)) / (n ** 2 * (n - 1))\n",
        "    \n",
        "    # The test statistic\n",
        "    z = (runs - expected_runs) / sqrt(variance_runs)\n",
        "    \n",
        "    # The p-value\n",
        "    p_value = erfc(abs(z) / sqrt(2))\n",
        "    \n",
        "    return p_value\n",
        "\n",
        "def linear_complexity(bit_string, M=500):\n",
        "    # Perform linear complexity test with block size M\n",
        "    n = len(bit_string)\n",
        "    bit_array = np.array([int(bit) for bit in bit_string])\n",
        "    lc = 0  # Initialize linear complexity\n",
        "    \n",
        "    # Process blocks of size M\n",
        "    for i in range(0, n, M):\n",
        "        block = bit_array[i:i+M]\n",
        "        if len(block) < M:\n",
        "            continue\n",
        "        \n",
        "        lc_block = 0\n",
        "        for j in range(M):\n",
        "            if block[j] == 1:\n",
        "                lc_block = j + 1\n",
        "        \n",
        "        lc += lc_block\n",
        "    \n",
        "    lc = lc / (n / M)\n",
        "    return lc\n",
        "\n",
        "def autocorrelation_test(bit_string, lag=1):\n",
        "    n = len(bit_string)\n",
        "    bit_array = np.array([int(bit) for bit in bit_string])\n",
        "    autocorrelation = np.correlate(bit_array, np.roll(bit_array, lag), mode='valid')[0]\n",
        "    return autocorrelation / n\n",
        "\n",
        "def maurer_universal_test(bit_string):\n",
        "    k = 6\n",
        "    l = 5\n",
        "    q = 20\n",
        "    bit_array = np.array([int(bit) for bit in bit_string])\n",
        "    max_val = 2**k\n",
        "    init_subseq = bit_array[:q]\n",
        "    rest_subseq = bit_array[q:]\n",
        "    d = {}\n",
        "    for i in range(len(init_subseq) - k + 1):\n",
        "        d[tuple(init_subseq[i:i+k])] = i\n",
        "    t = []\n",
        "    for i in range(len(rest_subseq) - k + 1):\n",
        "        subseq = tuple(rest_subseq[i:i+k])\n",
        "        if subseq in d:\n",
        "            t.append(i - d[subseq])\n",
        "            d[subseq] = i\n",
        "    if not t:\n",
        "        return 0\n",
        "    t = np.array(t)\n",
        "    log_avg = np.mean(np.log2(t))\n",
        "    return log_avg - np.log2(q)\n",
        "\n",
        "def binary_matrix_rank_test(bit_string, M=32, Q=32):\n",
        "    bit_array = np.array([int(bit) for bit in bit_string])\n",
        "    num_matrices = len(bit_array) // (M * Q)\n",
        "    ranks = []\n",
        "    for i in range(num_matrices):\n",
        "        matrix = bit_array[i*M*Q:(i+1)*M*Q].reshape((M, Q))\n",
        "        rank = np.linalg.matrix_rank(matrix)\n",
        "        ranks.append(rank)\n",
        "    return np.mean(ranks)\n",
        "\n",
        "def cumulative_sums_test(bit_string):\n",
        "    bit_array = np.array([int(bit) for bit in bit_string])\n",
        "    adjusted = 2 * bit_array - 1\n",
        "    cumulative_sum = np.cumsum(adjusted)\n",
        "    max_excursion = np.max(np.abs(cumulative_sum))\n",
        "    return max_excursion\n",
        "\n",
        "def longest_run_ones_test(bit_string, block_size=100):\n",
        "    bit_array = np.array([int(bit) for bit in bit_string])\n",
        "    num_blocks = len(bit_array) // block_size\n",
        "    max_runs = []\n",
        "    for i in range(num_blocks):\n",
        "        block = bit_array[i*block_size:(i+1)*block_size]\n",
        "        max_run = max([len(list(g)) for k, g in itertools.groupby(block) if k == 1])\n",
        "        max_runs.append(max_run)\n",
        "    return np.mean(max_runs)\n",
        "\n",
        "def random_excursions_test(bit_string):\n",
        "    bit_array = np.array([int(bit) for bit in bit_string])\n",
        "    bit_array = 2 * bit_array - 1  # Convert to Â±1\n",
        "\n",
        "    cumulative_sum = np.cumsum(bit_array)\n",
        "    states = np.unique(cumulative_sum)\n",
        "\n",
        "    if 0 not in states:\n",
        "        states = np.append(states, 0)\n",
        "    state_counts = {state: 0 for state in states}\n",
        "    for state in cumulative_sum:\n",
        "        state_counts[state] += 1\n",
        "\n",
        "    state_counts[0] -= 1  # Adjust for zero state\n",
        "    pi = [0.5 * (1 - (1 / (2 * state + 1)**2)) for state in states]\n",
        "    x = np.sum([(state_counts[state] - len(bit_string) * pi[i])**2 / (len(bit_string) * pi[i]) for i, state in enumerate(states)])\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def unique_subsequences(bit_string, length=4):\n",
        "    bit_array = np.array([int(bit) for bit in bit_string])\n",
        "    n = len(bit_array)\n",
        "    subsequences = set()\n",
        "    \n",
        "    for i in range(n - length + 1):\n",
        "        subseq = tuple(bit_array[i:i+length])\n",
        "        subsequences.add(subseq)\n",
        "    \n",
        "    return len(subsequences)\n",
        "\n",
        "def sample_entropy(bit_string, m=2, r=0.2):\n",
        "    bit_array = np.array([int(bit) for bit in bit_string])\n",
        "    N = len(bit_array)\n",
        "    \n",
        "    def _phi(m):\n",
        "        x = np.array([bit_array[i:i+m] for i in range(N - m + 1)])\n",
        "        C = np.sum(np.all(np.abs(x[:, None] - x) <= r, axis=2), axis=0) / (N - m + 1.0)\n",
        "        return np.sum(C) / (N - m + 1.0)\n",
        "    \n",
        "    return -np.log(_phi(m + 1) / _phi(m))\n",
        "\n",
        "def permutation_entropy(bit_string, order=3):\n",
        "    bit_array = np.array([int(bit) for bit in bit_string])\n",
        "    n = len(bit_array)\n",
        "    \n",
        "    permutations = np.array(list(itertools.permutations(range(order))))\n",
        "    c = np.zeros(len(permutations))\n",
        "    \n",
        "    for i in range(n - order + 1):\n",
        "        sorted_index_array = tuple(np.argsort(bit_array[i:i+order]))\n",
        "        for j, p in enumerate(permutations):\n",
        "            if np.array_equal(p, sorted_index_array):\n",
        "                c[j] += 1\n",
        "    \n",
        "    c = c / (n - order + 1)\n",
        "    pe = -np.sum(c * np.log2(c + np.finfo(float).eps))\n",
        "    return pe\n",
        "\n",
        "def lyapunov_exponent(bit_string, m=2, t=1):\n",
        "    bit_array = np.array([int(bit) for bit in bit_string])\n",
        "    N = len(bit_array)\n",
        "    \n",
        "    def _phi(m):\n",
        "        x = np.array([bit_array[i:i+m] for i in range(N - m + 1)])\n",
        "        C = np.sum(np.all(np.abs(x[:, None] - x) <= t, axis=2), axis=0) / (N - m + 1.0)\n",
        "        return np.sum(np.log(C + np.finfo(float).eps)) / (N - m + 1.0)\n",
        "    \n",
        "    return abs(_phi(m) - _phi(m + 1))\n",
        "\n",
        "def entropy_rate(bit_string, k=2):\n",
        "    bit_array = np.array([int(bit) for bit in bit_string])\n",
        "    n = len(bit_array)\n",
        "    prob = {}\n",
        "    \n",
        "    for i in range(n - k + 1):\n",
        "        subseq = tuple(bit_array[i:i + k])\n",
        "        if subseq in prob:\n",
        "            prob[subseq] += 1\n",
        "        else:\n",
        "            prob[subseq] = 1\n",
        "    \n",
        "    for key in prob:\n",
        "        prob[key] /= (n - k + 1)\n",
        "    \n",
        "    entropy_rate = -sum(p * log2(p) for p in prob.values())\n",
        "    return entropy_rate\n",
        "\n",
        "# Apply randomness tests\n",
        "def apply_randomness_tests(df, tests):\n",
        "    if not tests:\n",
        "        raise ValueError(\"No randomness tests specified.\")\n",
        "\n",
        "    test_functions = {\n",
        "        'autocorrelation': autocorrelation_test,\n",
        "        'cumulative_sums': cumulative_sums_test,\n",
        "        'spectral_test': classic_spectral_test,\n",
        "        'frequency_test': frequency_test,\n",
        "        'runs_test': runs_test,\n",
        "        'shannon_entropy': shannon_entropy,\n",
        "        'min_entropy': calculate_min_entropy\n",
        "    }\n",
        "\n",
        "    for test in tests:\n",
        "        if test not in test_functions:\n",
        "            raise ValueError(f\"Invalid randomness test: {test}\")\n",
        "        df[test] = df['Concatenated_Data'].apply(test_functions[test])\n",
        "\n",
        "    return df\n",
        "\n",
        "# Preprocess data\n",
        "def preprocess_data(df, num_concats, tests):\n",
        "    df = concatenateData(df, num_concats)\n",
        "    processed_df = apply_randomness_tests(df, tests)\n",
        "    \n",
        "    # Convert concatenated binary strings into separate columns\n",
        "    df_features = pd.DataFrame(processed_df['Concatenated_Data'].apply(list).tolist(), dtype=float)\n",
        "    processed_df = pd.concat([processed_df.drop(columns='Concatenated_Data'), df_features], axis=1)\n",
        "\n",
        "    return processed_df\n",
        "\n",
        "# Calculate min-entropy\n",
        "def calculate_min_entropy(sequence):\n",
        "    sequence = np.asarray(sequence, dtype=float)  # Convert sequence to float\n",
        "    p = np.mean(sequence)  # Proportion of ones\n",
        "    max_prob = max(p, 1 - p)\n",
        "    if max_prob == 0:  # Handle the case where all bits are the same\n",
        "        return 0\n",
        "    min_entropy = -np.log2(max_prob)\n",
        "    return min_entropy\n",
        "\n",
        "# Main\n",
        "file_path = 'AI_2qubits_training_data.txt'\n",
        "\n",
        "# Read the data from the file\n",
        "data = []\n",
        "with open(file_path, 'r') as file:\n",
        "    for line in file:\n",
        "        if line.strip():\n",
        "            binary_number, label = line.strip().split()\n",
        "            data.append((binary_number, int(label)))\n",
        "\n",
        "# Convert the data into a DataFrame\n",
        "df = pd.DataFrame(data, columns=['binary_number', 'label'])\n",
        "\n",
        "tests_to_apply = ['spectral_test', 'shannon_entropy', 'frequency_test', 'runs_test', 'autocorrelation', 'min_entropy']\n",
        "\n",
        "# Preprocess data and apply randomness tests\n",
        "preprocessed_df = preprocess_data(df, num_concats=1, tests=tests_to_apply)\n",
        "\n",
        "# Split the data into features (X) and labels (y)\n",
        "X = preprocessed_df.drop(columns='label').values\n",
        "y = preprocessed_df['label'].values.astype(int)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#test\n",
        "\n",
        "selector = SelectKBest(score_func=f_classif, k=90)\n",
        "X_train = selector.fit_transform(X_train, y_train)\n",
        "X_test = selector.transform(X_test)\n",
        "\n",
        "print(preprocessed_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Brik04cEFwK"
      },
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQm1VbnVEF3K"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "937q3D9VAzlM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Hyperparameters: {'C': 1, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'}\n",
            "Accuracy: 0.6814285714285714\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Perform hyperparameter tuning with GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': ['scale', 'auto', 0.1, 1, 10],\n",
        "    'kernel': ['linear', 'rbf', 'poly'],\n",
        "    'degree': [2, 3, 4],\n",
        "}\n",
        "\n",
        "svm_model = SVC(random_state=42)\n",
        "grid_search = GridSearchCV(svm_model, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMKS-cKqbkL0",
        "outputId": "f1e56f55-3340-423b-8fe5-ac06e95cb71c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Hyperparameters: {'kernel': 'linear', 'gamma': 'scale', 'C': 0.1}\n",
            "Accuracy: 0.6757142857142857\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Perform hyperparameter tuning with RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'gamma': ['scale', 'auto', 0.1, 1],\n",
        "    'kernel': ['linear', 'rbf'],\n",
        "}\n",
        "\n",
        "svm_model = SVC(random_state=42)\n",
        "random_search = RandomizedSearchCV(svm_model, param_distributions=param_dist, n_iter=5, cv=5)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and model\n",
        "best_params = random_search.best_params_\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDFlld25Xze5"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFkkQCBxXlWh",
        "outputId": "1c9794f0-a799-4de4-98e3-0dad406a3ad3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest Accuracy: 0.6107142857142858\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "def calculate_min_entropy(sequence):\n",
        "    sequence = np.asarray(sequence, dtype=float)  # Convert sequence to float\n",
        "    p = np.mean(sequence)  # Proportion of ones\n",
        "    max_prob = max(p, 1 - p)\n",
        "    if max_prob == 0:  # Handle the case where all bits are the same\n",
        "        return 0\n",
        "    min_entropy = -np.log2(max_prob)\n",
        "    return min_entropy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "vectorized_entropy = np.vectorize(calculate_min_entropy, signature='(n)->()')\n",
        "\n",
        "# Calculate min-entropy for each sequence in the training and testing datasets\n",
        "min_entropy_train = vectorized_entropy(X_train)\n",
        "min_entropy_test = vectorized_entropy(X_test)\n",
        "\n",
        "X_train_with_entropy = np.column_stack((X_train, min_entropy_train))\n",
        "X_test_with_entropy = np.column_stack((X_test, min_entropy_test))\n",
        "# Create the Random Forest classifier\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(X_train_with_entropy, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_rf = rf_model.predict(X_test_with_entropy)\n",
        "\n",
        "# Calculate the accuracy of the Random Forest model\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(\"Random Forest Accuracy:\", accuracy_rf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KULNfXpncJZC",
        "outputId": "f44dab80-2cb1-46c1-ab5f-a26725d94c9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest Accuracy: 0.6117857142857143\n",
            "Best Hyperparameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Create the Random Forest classifier\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid to search\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],          # Number of trees in the forest\n",
        "    'max_depth': [None, 10, 20, 30],         # Maximum depth of the tree\n",
        "    'min_samples_split': [2, 5, 10],         # Minimum number of samples required to split an internal node\n",
        "    'min_samples_leaf': [1, 2, 4],           # Minimum number of samples required to be at a leaf node\n",
        "    'max_features': ['sqrt'],       # Number of features to consider when looking for the best split\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV to find the best hyperparameters\n",
        "grid_search = GridSearchCV(rf_model, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "y_pred_rf = best_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the Random Forest model\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(\"Random Forest Accuracy:\", accuracy_rf)\n",
        "print(\"Best Hyperparameters:\", best_params)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DfNGSyaX3kV"
      },
      "source": [
        "## Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBUNIc9wAzvF",
        "outputId": "91db7722-33bb-4ea7-d39d-e0d6f484d698"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient Boosting Accuracy: 0.6278571428571429\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with mlflow.start_run():\n",
        "    # Initialize the model\n",
        "    gb_model = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "    # Train the model\n",
        "    gb_model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred_gb = gb_model.predict(X_test)\n",
        "\n",
        "    # Calculate the accuracy of the Gradient Boosting model\n",
        "    accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
        "\n",
        "    # Log parameters and metrics\n",
        "    mlflow.log_param(\"random_state\", 42)\n",
        "    mlflow.log_param(\"n_estimators\", gb_model.n_estimators)\n",
        "    mlflow.log_param(\"learning_rate\", gb_model.learning_rate)\n",
        "    mlflow.log_metric(\"accuracy\", accuracy_gb)\n",
        "\n",
        "    # Log the model\n",
        "    mlflow.sklearn.log_model(gb_model, \"gradient_boosting_model\")\n",
        "\n",
        "    # Print the accuracy\n",
        "    print(\"Gradient Boosting Accuracy:\", accuracy_gb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1L945YBW8hx"
      },
      "source": [
        "GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iD-fcbeW6NG",
        "outputId": "f25bbfd6-eb0e-4516-fbd3-ee1355c603f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Hyperparameters: {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 150}\n",
            "Gradient Boosting Accuracy: 0.6353571428571428\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "\n",
        "# Create the Gradient Boosting classifier\n",
        "gb_model = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid for Grid Search\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7],\n",
        "}\n",
        "\n",
        "# Perform Grid Search with cross-validation (cv=5) to find the best hyperparameters\n",
        "grid_search = GridSearchCV(gb_model, param_grid, cv=5)\n",
        "grid_search.fit(X_train_with_entropy, y_train)\n",
        "\n",
        "# Get the best hyperparameters and model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "y_pred_gb = best_model.predict(X_test_with_entropy)\n",
        "\n",
        "# Calculate the accuracy of the Gradient Boosting model with the best hyperparameters\n",
        "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(\"Gradient Boosting Accuracy:\", accuracy_gb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6rTZWuztjNr"
      },
      "source": [
        "## XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4qdLReQtmZ1",
        "outputId": "85c7db2a-2634-4203-b910-d940e2277d82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Hyperparameters for XGBoost: {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 300}\n",
            "XGBoost Accuracy: 0.11214285714285714\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "# Create the XGBoost classifier\n",
        "xgb_model = xgb.XGBClassifier(random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid for Grid Search\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7],\n",
        "}\n",
        "\n",
        "# Map classes to [0, 1, 2]\n",
        "y_train_mapped = y_train - 1  # This will change classes [1, 2, 3] to [0, 1, 2]\n",
        "\n",
        "# Continue with the Grid Search\n",
        "grid_search = GridSearchCV(xgb_model, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train_mapped)\n",
        "\n",
        "# Get the best hyperparameters and model\n",
        "best_params = grid_search.best_params_\n",
        "best_xgb_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set using the best XGBoost model\n",
        "y_pred_xgb = best_xgb_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the XGBoost model with the best hyperparameters\n",
        "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
        "print(\"Best Hyperparameters for XGBoost:\", best_params)\n",
        "print(\"XGBoost Accuracy:\", accuracy_xgb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_stlSoN3x3s"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVKGPCOT3x9q"
      },
      "source": [
        "## CatBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXVAMJrx3xLr",
        "outputId": "c38cd918-64d1-4d74-ede9-90354ac3b778"
      },
      "outputs": [],
      "source": [
        "import lightgbm as lgb\n",
        "\n",
        "# Create the LightGBM classifier\n",
        "lgb_model = lgb.LGBMClassifier(random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid for Grid Search\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7],\n",
        "}\n",
        "\n",
        "# Perform Grid Search with cross-validation (cv=5) to find the best hyperparameters\n",
        "grid_search = GridSearchCV(lgb_model, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and model\n",
        "best_params = grid_search.best_params_\n",
        "best_lgb_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set using the best LightGBM model\n",
        "y_pred_lgb = best_lgb_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the LightGBM model with the best hyperparameters\n",
        "accuracy_lgb = accuracy_score(y_test, y_pred_lgb)\n",
        "print(\"Best Hyperparameters for LightGBM:\", best_params)\n",
        "print(\"LightGBM Accuracy:\", accuracy_lgb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3RxycX8X8wX"
      },
      "source": [
        "## Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htyjktKSX-cN",
        "outputId": "04255ea4-c379-4f5f-9b09-8ff8dd70ef1e"
      },
      "outputs": [
        {
          "ename": "NotFittedError",
          "evalue": "This LabelEncoder instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[74], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Assuming you have already defined X_train, X_test, y_train, and y_test\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Convert binary numbers to integer labels\u001b[39;00m\n\u001b[1;32m     11\u001b[0m label_encoder \u001b[38;5;241m=\u001b[39m LabelEncoder()\n\u001b[0;32m---> 12\u001b[0m y_train_integer \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39mtransform(y_train)\n\u001b[1;32m     13\u001b[0m y_test_integer \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39mtransform(y_test)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Check unique values in y_train_integer and y_test_integer\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:133\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, y):\n\u001b[1;32m    121\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Transform labels to normalized encoding.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m        Labels as normalized encodings.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    134\u001b[0m     y \u001b[38;5;241m=\u001b[39m column_or_1d(y, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mdtype, warn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# transform of empty array is empty array\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1390\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1385\u001b[0m     fitted \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1386\u001b[0m         v \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(estimator) \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m v\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1387\u001b[0m     ]\n\u001b[1;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fitted:\n\u001b[0;32m-> 1390\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
            "\u001b[0;31mNotFittedError\u001b[0m: This LabelEncoder instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Assuming you have already defined X_train, X_test, y_train, and y_test\n",
        "\n",
        "# Convert binary numbers to integer labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_integer = label_encoder.transform(y_train)\n",
        "y_test_integer = label_encoder.transform(y_test)\n",
        "\n",
        "# Check unique values in y_train_integer and y_test_integer\n",
        "print(\"Unique values in y_train:\", np.unique(y_train_integer))\n",
        "print(\"Unique values in y_test:\", np.unique(y_test_integer))\n",
        "\n",
        "print(\"Shape of y_train_integer:\", y_train_integer.shape)\n",
        "print(\"Shape of y_test_integer:\", y_test_integer.shape)\n",
        "\n",
        "# Manually split the data into training and validation sets\n",
        "X_train, X_val, y_train_integer, y_val_integer = train_test_split(X_train, y_train_integer, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create the Neural Network model\n",
        "nn_model = Sequential()\n",
        "nn_model.add(Dense(32, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "nn_model.add(Dense(16, activation='relu'))\n",
        "nn_model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "nn_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "nn_model.fit(X_train, y_train_integer, epochs=2, batch_size=64, validation_data=(X_val, y_val_integer), verbose=0)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_probabilities = nn_model.predict(X_test)\n",
        "y_pred_nn = np.argmax(y_pred_probabilities, axis=-1)\n",
        "\n",
        "# Calculate the accuracy of the Neural Network model\n",
        "accuracy_nn = accuracy_score(y_test_integer, y_pred_nn)\n",
        "print(\"Neural Network Accuracy:\", accuracy_nn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGXQrE8d9d0x"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/100], Loss: 2.4247\n",
            "Epoch [2/100], Loss: 1.2638\n",
            "Epoch [3/100], Loss: 1.1338\n",
            "Epoch [4/100], Loss: 0.7737\n",
            "Epoch [5/100], Loss: 0.7216\n",
            "Epoch [6/100], Loss: 0.6942\n",
            "Epoch [7/100], Loss: 0.5081\n",
            "Epoch [8/100], Loss: 0.3536\n",
            "Epoch [9/100], Loss: 0.1385\n",
            "Epoch [10/100], Loss: 0.0303\n",
            "Epoch [11/100], Loss: 0.0343\n",
            "Epoch [12/100], Loss: 0.0135\n",
            "Epoch [13/100], Loss: 0.0086\n",
            "Epoch [14/100], Loss: 0.0066\n",
            "Epoch [15/100], Loss: 0.0045\n",
            "Epoch [16/100], Loss: 0.0037\n",
            "Epoch [17/100], Loss: 0.0039\n",
            "Epoch [18/100], Loss: 0.0025\n",
            "Epoch [19/100], Loss: 0.0027\n",
            "Epoch [20/100], Loss: 0.0025\n",
            "Epoch [21/100], Loss: 0.0020\n",
            "Epoch [22/100], Loss: 0.0019\n",
            "Epoch [23/100], Loss: 0.0014\n",
            "Epoch [24/100], Loss: 0.0016\n",
            "Epoch [25/100], Loss: 0.0016\n",
            "Epoch [26/100], Loss: 0.0012\n",
            "Epoch [27/100], Loss: 0.0017\n",
            "Epoch [28/100], Loss: 0.0013\n",
            "Epoch [29/100], Loss: 0.0012\n",
            "Epoch [30/100], Loss: 0.0009\n",
            "Epoch [31/100], Loss: 0.0009\n",
            "Epoch [32/100], Loss: 0.0007\n",
            "Epoch [33/100], Loss: 0.0009\n",
            "Epoch [34/100], Loss: 0.0008\n",
            "Epoch [35/100], Loss: 0.0007\n",
            "Epoch [36/100], Loss: 0.0008\n",
            "Epoch [37/100], Loss: 0.0009\n",
            "Epoch [38/100], Loss: 0.0006\n",
            "Epoch [39/100], Loss: 0.0004\n",
            "Epoch [40/100], Loss: 0.0005\n",
            "Epoch [41/100], Loss: 0.0006\n",
            "Epoch [42/100], Loss: 0.0005\n",
            "Epoch [43/100], Loss: 0.0006\n",
            "Epoch [44/100], Loss: 0.0005\n",
            "Epoch [45/100], Loss: 0.0004\n",
            "Epoch [46/100], Loss: 0.0005\n",
            "Epoch [47/100], Loss: 0.0005\n",
            "Epoch [48/100], Loss: 0.0005\n",
            "Epoch [49/100], Loss: 0.0004\n",
            "Epoch [50/100], Loss: 0.0003\n",
            "Epoch [51/100], Loss: 0.0005\n",
            "Epoch [52/100], Loss: 0.0003\n",
            "Epoch [53/100], Loss: 0.0005\n",
            "Epoch [54/100], Loss: 0.0003\n",
            "Epoch [55/100], Loss: 0.0003\n",
            "Epoch [56/100], Loss: 0.0003\n",
            "Epoch [57/100], Loss: 0.0004\n",
            "Epoch [58/100], Loss: 0.0004\n",
            "Epoch [59/100], Loss: 0.0003\n",
            "Epoch [60/100], Loss: 0.0003\n",
            "Epoch [61/100], Loss: 0.0003\n",
            "Epoch [62/100], Loss: 0.0002\n",
            "Epoch [63/100], Loss: 0.0003\n",
            "Epoch [64/100], Loss: 0.0003\n",
            "Epoch [65/100], Loss: 0.0002\n",
            "Epoch [66/100], Loss: 0.0003\n",
            "Epoch [67/100], Loss: 0.0002\n",
            "Epoch [68/100], Loss: 0.0002\n",
            "Epoch [69/100], Loss: 0.0002\n",
            "Epoch [70/100], Loss: 0.0002\n",
            "Epoch [71/100], Loss: 0.0003\n",
            "Epoch [72/100], Loss: 0.0002\n",
            "Epoch [73/100], Loss: 0.0002\n",
            "Epoch [74/100], Loss: 0.0002\n",
            "Epoch [75/100], Loss: 0.0002\n",
            "Epoch [76/100], Loss: 0.0002\n",
            "Epoch [77/100], Loss: 0.0002\n",
            "Epoch [78/100], Loss: 0.0002\n",
            "Epoch [79/100], Loss: 0.0001\n",
            "Epoch [80/100], Loss: 0.0001\n",
            "Epoch [81/100], Loss: 0.0001\n",
            "Epoch [82/100], Loss: 0.0002\n",
            "Epoch [83/100], Loss: 0.0002\n",
            "Epoch [84/100], Loss: 0.0002\n",
            "Epoch [85/100], Loss: 0.0001\n",
            "Epoch [86/100], Loss: 0.0001\n",
            "Epoch [87/100], Loss: 0.0001\n",
            "Epoch [88/100], Loss: 0.0001\n",
            "Epoch [89/100], Loss: 0.0001\n",
            "Epoch [90/100], Loss: 0.0001\n",
            "Epoch [91/100], Loss: 0.0001\n",
            "Epoch [92/100], Loss: 0.0001\n",
            "Epoch [93/100], Loss: 0.0001\n",
            "Epoch [94/100], Loss: 0.0001\n",
            "Epoch [95/100], Loss: 0.0001\n",
            "Epoch [96/100], Loss: 0.0001\n",
            "Epoch [97/100], Loss: 0.0001\n",
            "Epoch [98/100], Loss: 0.0001\n",
            "Epoch [99/100], Loss: 0.0001\n",
            "Epoch [100/100], Loss: 0.0001\n",
            "Accuracy of the model on the test set: 54.29%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Ensure reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc3(out)\n",
        "        return out\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size = 16\n",
        "num_classes = 4  # Labels are 1, 2, 3, 4\n",
        "model = MLP(input_size, hidden_size, num_classes)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for i, (features, labels) in enumerate(train_loader):\n",
        "        # Forward pass\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels - 1)  # labels - 1 to convert to 0-based index\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for features, labels in test_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == (labels - 1)).sum().item()  # labels - 1 to convert to 0-based index\n",
        "\n",
        "    print(f'Accuracy of the model on the test set: {100 * correct / total:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sE0n_hlbMAr",
        "outputId": "3cf01564-e8f0-431b-e490-48ab0299a741"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# ... (Previous code for reading and preprocessing the data)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Convert the data into numerical format\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m X_train \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     12\u001b[0m X_test \u001b[38;5;241m=\u001b[39m X_test\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     13\u001b[0m y_train \u001b[38;5;241m=\u001b[39m y_train\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# ... (Previous code for reading and preprocessing the data)\n",
        "\n",
        "# Convert the data into numerical format\n",
        "X_train = X_train.astype(np.float32)\n",
        "X_test = X_test.astype(np.float32)\n",
        "y_train = y_train.astype(np.float32)\n",
        "y_test = y_test.astype(np.float32)\n",
        "\n",
        "# Reshape the input data for LSTM\n",
        "time_steps = 1  # Each sample is treated as a single time step\n",
        "X_train_lstm = X_train.reshape(X_train.shape[0], time_steps, X_train.shape[1])\n",
        "X_test_lstm = X_test.reshape(X_test.shape[0], time_steps, X_test.shape[1])\n",
        "\n",
        "# Create the LSTM model\n",
        "lstm_model = Sequential()\n",
        "lstm_model.add(LSTM(32, input_shape=(time_steps, X_train.shape[1])))\n",
        "lstm_model.add(Dense(16, activation='relu'))\n",
        "lstm_model.add(Dense(3, activation='sigmoid'))  # Assuming binary classification\n",
        "\n",
        "# Compile the model\n",
        "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "lstm_model.fit(X_train_lstm, y_train, epochs=50, batch_size=32, verbose=1)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_lstm = lstm_model.predict(X_test_lstm)\n",
        "y_pred_lstm = np.round(y_pred_lstm).astype(int).flatten()  # Convert probabilities to binary predictions\n",
        "\n",
        "# Calculate the accuracy of the LSTM model\n",
        "accuracy_lstm = accuracy_score(y_test, y_pred_lstm)\n",
        "print(\"LSTM Accuracy:\", accuracy_lstm)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
